{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 1 - ML - Grundverfahren SS 23\n",
    "\n",
    "Ge Li ge.li@kit.edu. This homework has totally 13 points.\n",
    "\n",
    "## Submission Instructions\n",
    "Submission deadline: May 17, 2023, 15:00. Please follow the instruction from homework ZERO! \n",
    "\n",
    "\n",
    "## 1.) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Matrix Vector Calculus (1 Point)\n",
    "Given the following element-wise expression of a matrix-vector product,\n",
    "rewrite it in matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "        g = \\alpha \\sum_i \\sum_j \\sum_k z_k x_{ij} q_i y_{jk}\n",
    "\\end{align*}\n",
    "\n",
    "where $g$ is a scalar, $\\alpha$ is a scalar, $x$ is a matrix, $y$ is a matrix, $z$ is a vector, $q$ is a vector, $i$ is an index for the rows of $x$ and $q$, $j$ is an index for the columns of $x$ and $y$, and $k$ is an index for the elements of $z$ and $y$.\n",
    "\n",
    "Now, we can rewrite the expression in matrix form:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Derive Ridge Regression Weights (4 Points)\n",
    "Derive the optimal solution of weights in Ridge Regression using matrix form, i\n",
    ".e. $\\boldsymbol{w}= ?$\n",
    "\n",
    "Hint: You will need derivatives for vectors/matrices. Start\n",
    "from the matrix objective for ridge regression as stated here\n",
    "\n",
    "\\begin{align*}\n",
    "L &= (\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w})^T(\\boldsymbol{y}-\\boldsymbol{\\Phi} \\boldsymbol{w}) + \\lambda \\boldsymbol{w}^T \\boldsymbol{I} \\boldsymbol{w}. \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - Code\n",
    "Let's first get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Load data\n",
    "\n",
    "training_data = np.load('training_data.npy')\n",
    "test_data = np.load('test_data.npy')\n",
    "\n",
    "test_data_x = test_data[:, 0]\n",
    "test_data_y = test_data[:, 1]\n",
    "\n",
    "training_data_x = training_data[:, 0]\n",
    "training_data_y = training_data[:, 1]\n",
    "\n",
    "# Visualize data\n",
    "plt.plot(test_data_x, test_data_y, 'or')\n",
    "plt.plot(training_data_x, training_data_y, 'ob')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend([\"test_data\", \"training data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the lecture notebook, we will use polynomial-features here again.\n",
    "The following functions will be used for:\n",
    "- calculating polynomial features\n",
    "- computing the mean and std of the features (training data) as normalizer\n",
    "- normalize other data (test) features using the normalizer (mean and std)\n",
    "- evaluating the model\n",
    "- calculating the Mean Squarred Error for assigning a performance to each\n",
    "model. <br><br>\n",
    "\n",
    "Note we will use the mean and the standard deviation to normalize our features\n",
    "according to:\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{\\tilde{\\Phi}} = \\frac{\\boldsymbol{\\Phi}(\\boldsymbol{x}) - \\boldsymbol{\\mu}_{\\Phi}}{\\boldsymbol{\\sigma}_{\\Phi}}, \n",
    "\\end{align*}\n",
    "where $\\boldsymbol{\\tilde{\\Phi}}$ are the (approximately) normalized features to any input\n",
    "$\\boldsymbol{x}$ (not necessarily the training data), $\\boldsymbol{\\mu}_{\\Phi}$ is the mean of the features applied to the training data and $\\boldsymbol{\\sigma}_{\\Phi}$ is the standard deviation of the features applied to the training data for each dimension.<br>\n",
    "\n",
    "Normalization is a standard technique used in Regression to avoid numerical problems and to obtain better fits for the weight vectors $\\boldsymbol{w}$. Especially when the features transform the inputs to a very high value range, normalization is very useful. In this homework we will use features of degree 10. Since the input range of the data is roughly from -4 to 4 this will lead to very high values for higher order degrees. By normalizing each dimension of the feature matrix, we will map each dimension of the feature matrix applied to the training data to a zero mean unit variance distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_features(data: np.ndarray,\n",
    "                            degree: int) ->np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to create Feature Matrix. Extends the feature matrix according to\n",
    "    the matrix form discussed in the lectures.\n",
    "\n",
    "    :param data: data points you want to evaluate the polynomials,\n",
    "                 shape: [n_samples] (we have 1-dim data)\n",
    "    :param degree: degree of your polynomial, shape: scalar\n",
    "    :return polynomial_features: shape [n_samples x (degree+1)]\n",
    "    \"\"\"\n",
    "    polynomial_features = np.ones(data.shape)\n",
    "    for i in range(degree):\n",
    "        polynomial_features = np.column_stack((polynomial_features, data ** (i + 1)))\n",
    "    return polynomial_features\n",
    "\n",
    "\n",
    "def get_mean_std_features(polynomial_features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Function for calculating the mean and standard deviation of the features\n",
    "    :param polynomial_features: shape: [n_samples x (degree+1)]\n",
    "    :return mean_feat: mean vector of the features,\n",
    "            shape:[1 x (degrees+1)]\n",
    "    :return std_feat: standard deviation (for each dimension in feature matrix),\n",
    "                      shape: [1 x (degrees+1)] \n",
    "    \"\"\"\n",
    "    mean_feat = np.mean(polynomial_features, axis=0, keepdims=True)\n",
    "    mean_feat[:, 0] = 0.0 # we don't want to normalize the bias\n",
    "    std_feat = np.std(polynomial_features, axis=0, keepdims=True)\n",
    "    std_feat[:, 0] = 1.0 # we don't want to normalize the bias\n",
    "    return mean_feat, std_feat\n",
    "\n",
    "\n",
    "def normalize_features(polynomial_features: np.ndarray,\n",
    "                       mean_train_features: np.ndarray,\n",
    "                       std_train_features: np.ndarray) ->np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize features\n",
    "    :param polynomial_features:  features to be normalized,\n",
    "                 shape: [n_samples x (degree+1)]\n",
    "    :param mean_train_features: mean of the feature matrix of the training set,\n",
    "                 shape: [1 x (degrees+1)]\n",
    "    :param std_train_features: std of the feature matrix of the training set,\n",
    "                 shape: [1 x (degrees+1)]\n",
    "    :return norm_feat: normalized features, shape: [n_samples x (degree+1)]\n",
    "    \"\"\"\n",
    "\n",
    "    # note: features: (n_samples x n_dims),\n",
    "    #       mean_train_features: (1 x n_dims),\n",
    "    #       std_train_features:  (1 x n_dims)\n",
    "    #       due to these dimensionalities we can do element-wise operations.\n",
    "    #       By this we normalize each dimension independently\n",
    "    norm_feat = (polynomial_features - mean_train_features) / std_train_features\n",
    "    return norm_feat\n",
    "\n",
    "\n",
    "def eval(Phi:np.ndarray, w:np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate the models\n",
    "\n",
    "    :param Phi: Feature matrix, shape: [n_samples x (degree+1)]\n",
    "    :param w: weight vector, shape: [degree + 1]\n",
    "    :return : predictions, shape [n_samples] (we have 1-dim data)\n",
    "    Evaluates your model\n",
    "    \"\"\"\n",
    "    return np.dot(Phi, w)\n",
    "\n",
    "\n",
    "def mse(y_target:np.ndarray, y_pred:np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    :param y_target: the target outputs,\n",
    "            shape: [n_samples] (here 1-dim data)\n",
    "    :param y_pred: the predicted outputs,\n",
    "            shape: [n_samples](we have 1-dim data)\n",
    "    :return : The Mean Squared Error, shape: scalar\n",
    "    \"\"\"\n",
    "    diff = y_target - y_pred\n",
    "    return np.sum(diff ** 2, axis=0) / y_pred.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Ridge Regression Weights (2 Point)\n",
    "The following function will calculate the weights for ridge regression. Fill in the missing code according to the formula for calculating the weight updates for ridge regression. <br>\n",
    "Recall that the formula is given by \n",
    "\\begin{align*}\n",
    "    \\boldsymbol{w} &= (\\boldsymbol{\\Phi} ^T \\boldsymbol{\\Phi} + \\lambda \\boldsymbol{I} )^{-1} \\boldsymbol{\\Phi}^T \\boldsymbol{y},\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{\\Phi}$ is the feature matrix (the matrix storing the data points applied to the polynomial features).\n",
    "Hint: use np.linalg.solve for solving for the linear equation.\n",
    "If you got confused because of the normalization described before, don't worry, you do not need to consider it here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights_ridge(Phi:np.ndarray,\n",
    "                       y:np.ndarray,\n",
    "                       ridge_factor:float)->np.ndarray:\n",
    "    \"\"\"\n",
    "    :param Phi: Feature Matrix, shape: [n_samples x (degree+1)]\n",
    "    :param y: Output Values, [n_samples] (we have 1-dim data)\n",
    "    :param ridge_factor: lambda value, shape: scalar\n",
    "    :return : The weight vector, calculated according to the equation shown before,\n",
    "            shape: [degrees +1]\n",
    "    \"\"\"\n",
    "    ##################\n",
    "    ##TODO\n",
    "    #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstrating ridge regression we will pick the polynomial degree of 10. In the lecture notebook we have seen that this model is highly overfitting to the data.\n",
    "We will investigate the role of the ridge factor $\\lambda$. For that purpose we first need to calculate the weights for different $\\lambda$ values. <br>\n",
    "We will pick $\\lambda = [1e-{6}, 1e-{3}, 1, 3, 5,10,20,30,40,50, 1e2, 1e3, 1e5] $ to see the differences of the values. <br><br>\n",
    "\n",
    "Practical note. We use here very high values for $\\lambda$ for demonstration\n",
    "purposes here. In practice we would not choose a model where we know from\n",
    "beginning that it is highly overfitting. When choosing an appropriate model, the value needed for $\\lambda$ automatically will be small (often in the range of $1e^{-6}$ or smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do it on polynomial degree 10 and see the results\n",
    "\n",
    "# first we get the mean and the standard deviation of the training feature matrix, which we will use for normalization\n",
    "train_features = get_polynomial_features(training_data_x, 10)\n",
    "test_features = get_polynomial_features(test_data_x, 10)\n",
    "mean_train_feat, std_train_feat = get_mean_std_features(train_features)\n",
    "norm_train_features = normalize_features(train_features, mean_train_feat, std_train_feat)\n",
    "norm_test_features = normalize_features(test_features, mean_train_feat, std_train_feat)\n",
    "\n",
    "\n",
    "# now we can calculate the normalized features for degree 10\n",
    "ridge_factors = [1e-6, 1e-3, 1, 3, 5, 10,20,30,40, 50, 1e2, 1e3, 1e5]\n",
    "weights_ridge = []\n",
    "\n",
    "for lambda_val in ridge_factors:\n",
    "    weights_ridge.append(calc_weights_ridge(norm_train_features, training_data_y, lambda_val))\n",
    "\n",
    "# We further have to perform the predictions based on the models we have calculated\n",
    "y_training_ridge = []\n",
    "y_test_ridge = []\n",
    "\n",
    "for w in weights_ridge:\n",
    "    y_training_ridge.append(eval(norm_train_features, w))\n",
    "    y_test_ridge.append(eval(norm_test_features, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the mean squarred error on the test and the training data. For that purpose we calculate them here and plot the errors for different $\\lambda$ values in log space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error_ridge = []\n",
    "test_error_ridge = []\n",
    "\n",
    "for i in range(len(y_training_ridge)):\n",
    "    training_error_ridge.append(mse(training_data_y, y_training_ridge[i]))\n",
    "    test_error_ridge.append(mse(test_data_y, y_test_ridge[i]))\n",
    "\n",
    "error_fig_ridge = plt.figure()\n",
    "plt.figure(error_fig_ridge.number)\n",
    "plt.title(\"Error Plot Ridge Regression\")\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "x_axis = [\"$1e-{6}$\", \"$1e-{3}$\", \"$1$\", \"$3$\", \"$5$\",\"$10$\",\"$20$\",\"$30$\",\"$40$\",\"$50$\",\n",
    "          \"$1e2$\", \"$1e3$\", \"$1e5$\"]\n",
    "plt.yscale('log')\n",
    "plt.plot(x_axis, training_error_ridge, 'b')\n",
    "plt.plot(x_axis, test_error_ridge, 'r')\n",
    "# let's find the index with the minimum training error\n",
    "min_error_idx = np.argmin(test_error_ridge)\n",
    "plt.plot(x_axis[min_error_idx], test_error_ridge[min_error_idx], 'xg')\n",
    "plt.legend(['Training Error', 'Test Error', 'Min Test Error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us visualize the newly fitted model with the optimal lambda value here\n",
    "x = np.linspace(-5, 5, 100)\n",
    "new_features = get_polynomial_features(x, 10)\n",
    "new_norm_feat = normalize_features(new_features, mean_train_feat, std_train_feat)\n",
    "y_pred = eval(new_norm_feat, weights_ridge[min_error_idx])\n",
    "\n",
    "plt.plot()\n",
    "plt.plot(test_data_x, test_data_y, 'or')\n",
    "plt.plot(training_data_x, training_data_y, 'ob')\n",
    "plt.plot(x, y_pred)\n",
    "plt.legend([\"test_data\", \"training_data\", \"inference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4) Error Plot (1 Point)\n",
    "In the lecture we have seen and analyzed the plot of polynomial degrees \n",
    "against the error (slide  39).\n",
    "Similarly, now please analyze the relationship between the error and the \n",
    "different values of $\\lambda$, as well as the reason behind it.\n",
    "\n",
    "Hint: Do not forget that we are in log space. Small changes in the y-axis mean high differences in the error values. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Kernel Ridge Regression\n",
    "\n",
    "In this task, we are going to get familiar with the kernel method and perform Kernel Ridge Regression using Gaussian kernels.\n",
    "\n",
    "Work flow:\n",
    "- Load and plot data\n",
    "- Implement a function to get the Gaussian kernel vector\n",
    "- Implement a function to get the Gaussian kernel matrix\n",
    "- Implement a function to apply Kernel Ridge Regression\n",
    "- Select best model and see some result plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and plot data\n",
    "First, let us load and plot our data. Note that in this example the test data is not corrupted by noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed to obtain reproducible results\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load training, validation and test datasets\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "x_valid = np.load('x_valid.npy')\n",
    "y_valid = np.load('y_valid.npy')\n",
    "\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(dpi=200, tight_layout=True)\n",
    "plt.plot(x_train, y_train, 'ob', label='training data', zorder=20)\n",
    "plt.plot(x_valid, y_valid, 'or', label='validation data',zorder=10)\n",
    "plt.plot(x_test, y_test, 'ok', label='test data', alpha=0.3, zorder=0)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Training, Validation and Testing datasets (1pt)\n",
    "In this homework, we are going to use a new type of dataset \"validation\" to help select our best hyperparameter and avoid overfitting. Please read the explanation below and understand the roles of training, validation and testing datasets respectively (1pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and test datasets are three distinct subsets of a larger dataset that are used in the process of building and evaluating machine learning models.\n",
    "\n",
    "The training dataset is the subset of the data that is used to train a machine learning model. In other words, it's the data that the model learns from. Typically, the training dataset makes up the majority of the available data.\n",
    "\n",
    "The validation dataset is used to evaluate the performance of a model during the training process. It's a subset of the data that is held back from the training process and used to tune the model's hyperparameters, such as learning rate, number of hidden layers, etc. The goal of the validation dataset is to help prevent overfitting, which is when the model memorizes the training data and performs poorly on new, unseen data.\n",
    "\n",
    "The test dataset is a completely separate subset of the data that is used to evaluate the final performance of a trained model. It represents real-world data that the model has never seen before and is used to estimate the model's accuracy on new, unseen data. The test dataset should only be used after the model has been trained and validated to prevent any bias in the evaluation.\n",
    "\n",
    "It's important to note that the three datasets should be independent and identically distributed (IID) to ensure that the model is able to generalize well to new data. In addition, the sizes of the datasets may vary depending on the size and complexity of the original dataset and the specific requirements of the machine learning problem at hand.\n",
    "\n",
    "(Note: In our homework, however, we are going to use a large and denoised test dataset which is for plotting convienence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Kernels:\n",
    "Since Gaussian kernels are the most commonly used kernels, we will concentrate on Gaussian kernels in this notebook. Remember the definition of a Gaussian kernel (slide 55):\\\n",
    "\\begin{align*}\n",
    "    k(\\boldsymbol{x}, \\boldsymbol{x^*}) = \\exp\\left(-\\frac{||\\boldsymbol{x}-\\boldsymbol{x^*}||^2}{2\\sigma^2}\\right),\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{x}, \\boldsymbol{x^*}$ are two $d$-dimensional data points. $ \\sigma $ is the bandwidth hyperparameter. Recall that any kernel $k(\\boldsymbol{x}, \\boldsymbol{x^*})$ returns a scalar which represents some kind of discrepancy measure between two data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Kernel Vector (3pts)\n",
    "Now we are going to implement a function to compute the kernel vector $\\boldsymbol k(\\boldsymbol x^*)$ (slide 59). Recall the definition: given $N$ training points $\\boldsymbol{x_i}, i=1,...,N$, and one additional query point $\\boldsymbol{x^*}$, $\\boldsymbol{k(x^*)}$ is defined as the $N$-dimensional vector whose $i$-th element is given by the kernel, evaluated at training point $\\boldsymbol{x_i}$ and the query point, i.e. $k(\\boldsymbol{x_i, x^*})$.\n",
    "\n",
    "Please finish the function below!\n",
    "\n",
    "Hints:\n",
    "- As we typically have $M > 1$ query points in practice, we would like to compute the corresponding $M$ kernel vectors in one function call, cf. the comments.\n",
    "- The computations we have to perform are the same as in `gaussian_kernel()`. However, we are now operating on more than one input vectors at once.\n",
    "- Avoid using loops! To this end, you may need to add additional dimensions to the input data and use broadcasting. Also, note that numpy operations such as `np.linalg.norm()` can operate on vector inputs and take an `axis`-argument!\n",
    "- Make sure your code works also for data with dimension larger than 1. You are going to need this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_vec(X_t: np.ndarray,\n",
    "                   X_q: np.ndarray,\n",
    "                   sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X_t: N training inputs (shape: [N, d])\n",
    "    :param X_q: M query inputs (shape: [M, d])\n",
    "    :param sigma: bandwidth of the kernel (shape: scalar)\n",
    "    :return: M kernel vectors arranged as the columns of a matrix with shape [N, M]\n",
    "    \"\"\"\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "\n",
    "    # assert the output shape being [N, M]\n",
    "    assert list(kernel_vectors.shape) == [X_t.shape[0], X_q.shape[0]]\n",
    "    return kernel_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF) features\n",
    "Now we use `get_kernel_vec()` to compute the kernel vectors w.r.t. our training inputs, using the test inputs as the query points. This allows us to plot the RBF features: we observe that at each training data point a Gaussian bump (the Gaussian kernel) is centered and that the bandwith parameter specifies the width of each bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to test your code\n",
    "\n",
    "# Compute the kernel vectors w.r.t. the training inputs, using the test inputs as query points\n",
    "kernel_vectors = get_kernel_vec(x_train, x_test, 0.5)\n",
    "kernel_vectors2 = get_kernel_vec(x_train, x_test, 0.2)\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(dpi=200, tight_layout=True)\n",
    "\n",
    "# Plot the data\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_train, y_train, 'ro')\n",
    "plt.plot(x_test, y_test, 'k')\n",
    "plt.legend(['training data', 'ground_truth'])\n",
    "\n",
    "# Plot the kernel vectors\n",
    "plt.subplot(3,1,2)\n",
    "for kernel in kernel_vectors:\n",
    "    plt.plot(x_test, kernel, 'b')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "for kernel in kernel_vectors2:\n",
    "    plt.plot(x_test, kernel, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression with Gaussian Kernels\n",
    "\n",
    "Let's go ahead and do regression using Gaussian kernels. Remember the regression equation (slide 67):\n",
    "\n",
    "\\begin{align*}\n",
    "    f(\\boldsymbol{x^*}) = \\boldsymbol{k}(\\boldsymbol{x^*})^T(\\boldsymbol{K} + \\lambda \\boldsymbol{I})^{-1}\\boldsymbol{y},\n",
    "\\end{align*}\n",
    "where $ \\boldsymbol{k}(\\boldsymbol{x^*}) $ is the kernel vector, $ \\boldsymbol{K}$ is the kernel matrix and $ \\boldsymbol{y} $ are the target values from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kernel Matrix (1pt)\n",
    "First, we have to compute the kernel matrix $\\boldsymbol K$ (slide 59). Recall the definition: $[\\boldsymbol K]_{ij} = k(\\boldsymbol x_i, \\boldsymbol x_j)$ with our training inputs $\\boldsymbol{x_i}, i=1,...,N$.\n",
    "\n",
    "Implement the function below (hint: re-use the `get_kernel_vec()` function)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_mat(X: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X: N training inputs (shape [N, d])\n",
    "    :sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the kernel matrix (shape [N, N])\n",
    "    \"\"\"\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "    # assert the output shape being [N, N]\n",
    "    assert list(kernel_mat.shape) == [X.shape[0], X.shape[0]]\n",
    "    return kernel_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kernel Regression Prediction (2pts)\n",
    "Now that we have finished the implementation of the kernel vector $\\boldsymbol{k(x^*)}$ and the kernel matrix $\\boldsymbol{K}$, we can compute the prediction $f(\\boldsymbol{x^*})$ in the function in below. Here we choose a fixed ridge factor $ \\lambda = 10^{-3}$.\n",
    "\n",
    "Hint: make sure you use numerically stable operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_t: np.ndarray, y_t: np.ndarray,\n",
    "            X_q: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X_t: N training inputs (shape [N, d])\n",
    "    :param y_t: N training targets (shape [N, 1])\n",
    "    :param X_q: M query inputs (shape [M, d])\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: predicted values at X_q (shape [M, 1])\n",
    "    \"\"\"\n",
    "    k = get_kernel_vec(X_t, X_q, sigma) # shape [N, M]\n",
    "    K = get_kernel_mat(X_t, sigma) # shape [N, N]\n",
    "    ridge_factor = 1e-3\n",
    "    ############## Your code starts here ##############\n",
    "\n",
    "    ############### Your code ends here ###############\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "Once again, we will use the mean squared error to measure the training and test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_target: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param y_target: true y values\n",
    "    :param y_pred: predicted y values\n",
    "    :return: MSE\n",
    "    \"\"\"\n",
    "    return np.sum((y_target-y_pred)**2)/y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "The Gaussian kernel method introduces a hyperparameter (the bandwidth $\\sigma$). We have to determine its value via model selection. Here, we use the hold-out method, i.e., we try different $\\sigma$ values and select the model which performs best on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.01, .1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10]\n",
    "\n",
    "# compute train and validation errors for the different sigma values\n",
    "train_errors = []\n",
    "validate_errors = []\n",
    "for sigma in sigmas:\n",
    "    predict_train = predict(X_t=x_train, y_t=y_train,\n",
    "                              X_q=x_train, sigma=sigma)\n",
    "    predict_valid = predict(X_t=x_train, y_t=y_train,\n",
    "                             X_q=x_valid, sigma=sigma)\n",
    "    train_errors.append(mse(y_train, predict_train))\n",
    "    validate_errors.append(mse(y_valid, predict_valid))\n",
    "\n",
    "# determine best model\n",
    "min_valid_error_index = validate_errors.index(min(validate_errors))\n",
    "print(f'minimum validation error for sigma = {sigmas[min_valid_error_index]:.2f}')\n",
    "\n",
    "# plot the errors\n",
    "x_axis = [str(x) for x in sigmas]\n",
    "error_plot = plt.figure(dpi=200, tight_layout=True)\n",
    "plt.plot(x_axis, train_errors, 'b')\n",
    "plt.plot(x_axis, validate_errors, 'r')\n",
    "plt.plot(x_axis[min_valid_error_index], validate_errors[min_valid_error_index], 'gx')\n",
    "plt.xlabel( \"$\\sigma$\")\n",
    "plt.ylabel( \"MSE\" )\n",
    "plt.legend(['train error', 'validation error','min error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe the typical behavior: we have clear overfitting for small $ \\sigma $ values (high model complexity). For high $ \\sigma $ values (low model complexity) we observe underfitting. The best performing model is marked with a green 'x', as the minimum validation error is achieved for $ \\sigma = 3 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at predictions for different $ \\sigma $ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions for various bandwith parameters\n",
    "for sigma in [5.0,  3.0, 1.0 , 0.1]:\n",
    "    # compute predictions on train, test, and validation sets\n",
    "    y_pred_train = predict(X_t=x_train, y_t=y_train, X_q=x_train, sigma=sigma)\n",
    "    y_pred_valid = predict(X_t=x_train, y_t=y_train, X_q=x_valid, sigma=sigma)\n",
    "    y_pred_test = predict(X_t=x_train, y_t=y_train, X_q=x_test, sigma=sigma)\n",
    "\n",
    "    # print MSEs\n",
    "    print(f'sigma = {sigma:.2f}')\n",
    "    print(f'MSE train: {mse(y_train, y_pred_train):.2f}')\n",
    "    print(f'MSE valid: {mse(y_valid, y_pred_valid):.2f}')\n",
    "    print(f'MSE test : {mse(y_test, y_pred_test):.2f}')\n",
    "\n",
    "    # plot predictions\n",
    "    sigma_fig = plt.figure(dpi=75, tight_layout=True)\n",
    "    plt.plot(x_train, y_train, 'ro')\n",
    "    plt.plot(x_test, y_test, 'k')\n",
    "    plt.plot(x_test, y_pred_test, 'y')\n",
    "    plt.legend(['Training data', 'Ground-truth', '$\\sigma$ =' + str(sigma)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe that the model overfits for $ \\sigma < 3.0 $ and underfits for $\\sigma > 3.0$, with $\\sigma = 3.0$ achieving the minimum validation error.\n",
    "\n",
    "Note that the model prediction reverts back to $0$ far away from training data (where \"far away\" has to be understood relative to the bandwidth $\\sigma$). E.g., for $ \\sigma = 0.1 $, the model prediction reverts back to $0$ in the input region [-2.5, -1.5] as all training points are \"far away\" w.r.t. the small bandwith value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
